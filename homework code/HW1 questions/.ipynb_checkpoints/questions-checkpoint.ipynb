{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269a7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c505cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccbb00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "img_folder = 'imgs'\n",
    "if not os.path.exists(img_folder):\n",
    "    os.makedirs(img_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03b287",
   "metadata": {},
   "source": [
    "## Question 1: Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedff5e8",
   "metadata": {},
   "source": [
    "### preamble:\n",
    "<font size=\"3\">First load the separable dataset of 200 instances of 2D-features. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_samples\n",
    "\n",
    "# load the dataset\n",
    "data_q1_separable = np.load('datasets/q1_separable.npz')\n",
    "separable_x = data_q1_separable['x']\n",
    "separable_y = data_q1_separable['y']\n",
    "\n",
    "# visualize the datapoints\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, separable_x, separable_y, 'separable samples', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51c737",
   "metadata": {},
   "source": [
    "### Perceptron Algorithm without Maximizing the Margin\n",
    "<font size=\"3\"> **Q1(a)** The basic perceptron algorithm is already implemented. Complete all TODO part in q1.py marked as 'Q1(a)' to implement the computation of geometric margin. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b2348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q1 import perceptron_algo\n",
    "\n",
    "geom_margins = []\n",
    "result = perceptron_algo(separable_x, separable_y, N_iters=10, geom_margins=geom_margins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b09626",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(geom_margins)\n",
    "print(f'max margin: {geom_margins[-1]}')\n",
    "plt.savefig(os.path.join(img_folder, 'Q1_a.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0678a93",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **[Question]**: after how many iterations does the decision boundary stop updating?\n",
    "<br> **[Answer]**: \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc001f",
   "metadata": {},
   "source": [
    "#### Q1(b) Margin Perceptron Algorithm\n",
    "\n",
    "<font size=\"3\"> In this section, please complete the margin perceptron algorithm (TODO marked as 'Q1(b)' in q1.py). After you have completed the task, run the block below and answer the question. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42199c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_line\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_samples(axs[0], separable_x, separable_y, 'decision boundaries')\n",
    "axs[1].set_title('geometric margin')\n",
    "margins = [0.05, 0.1, 0.2, 0.5]\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "for margin, color in zip(margins, colors):\n",
    "    geom_margins = []\n",
    "    result = perceptron_algo(separable_x, separable_y, N_iters=10, geom_margins=geom_margins, margin=margin)\n",
    "    plot_line(axs[0], label=f'margin: {margin}', color=color, weights=result)\n",
    "    axs[1].plot(geom_margins, label=f'margin: {margin}', c=color)\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.savefig(os.path.join(img_folder, 'Q1_b.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f32723",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **[Question]**: Briefly discuss and explain the impact of $\\gamma$ on the resulting margin.\n",
    "<br> **[Answer]**: \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884f194a",
   "metadata": {},
   "source": [
    "### Q1(c)  Variable-Increment Perceptron\n",
    "\n",
    "<font size=\"3\"> In this section, please complete the variant-increment perceptron algorithm (TODO marked as 'Q1(c)' in q1.py). After you have completed the task, run the block below and answer the question. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05390636",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_samples(axs[0], separable_x, separable_y, 'decision boundaries')\n",
    "axs[1].set_title('geometric margin')\n",
    "margins = [0.05, 0.1, 0.2, 0.5]\n",
    "colors = ['red', 'orange', 'green', 'blue']\n",
    "for margin, color in zip(margins, colors):\n",
    "    geom_margins = []\n",
    "    result = perceptron_algo(\n",
    "        separable_x, separable_y, N_iters=10, geom_margins=geom_margins, \n",
    "        margin=margin, variable_increment=True)\n",
    "    plot_line(axs[0], label=f'margin: {margin}', color=color, weights=result)\n",
    "    axs[1].plot(geom_margins, label=f'margin: {margin}', c=color)\n",
    "axs[0].legend()\n",
    "axs[1].legend()\n",
    "plt.savefig(os.path.join(img_folder, 'Q1_c.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae46a5e",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **[Question]**: What step size $\\eta_i$ you use for each sample $x_i$. Explain how these step sizes satisfy the properties that guarantee a solution with the margin $\\frac{\\gamma}{2}$ on $\\gamma$-linearly separable dataset.\n",
    "<br> **[Answer]**: \n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> **[Question]**: Compare the result with the result obtained in (b). Describe and explain the difference.\n",
    "<br> **[Answer]**: \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61117082",
   "metadata": {},
   "source": [
    "### Data that are not linearly separable\n",
    "\n",
    "<font size=\"3\"> We will use a not linearly separable dataset with offset for Q1(d) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea617e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sparable samples\n",
    "data_q1_not_separable = np.load('datasets/q1_non_separable.npz')\n",
    "not_separable_x = data_q1_not_separable['x']\n",
    "not_separable_y = data_q1_not_separable['y']\n",
    "\n",
    "# visualize the datapoints\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, not_separable_x, not_separable_y, 'not linearly separable samples', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db52da",
   "metadata": {},
   "source": [
    "### Q1(d) Batch Variable-Increment Perceptron\n",
    "<font size=\"3\"> In this section, please complete the batch perceptron algorithm (TODO marked as 'Q1(d)' in q1.py). After you have completed the task, run the block below and answer the question. Boundary lines of darker color are generated in later iterations. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253b4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_samples(axs[0], not_separable_x, not_separable_y, 'online learning')\n",
    "plot_samples(axs[1], not_separable_x, not_separable_y, 'batch learning')\n",
    "theta_online = None\n",
    "theta_batch = None\n",
    "\n",
    "N_iters = 10\n",
    "interval = 3\n",
    "online_cmap = plt.get_cmap('Greens')\n",
    "batch_cmap = plt.get_cmap('Blues')\n",
    "for iter in range(N_iters):\n",
    "    theta_online = perceptron_algo(\n",
    "        not_separable_x, not_separable_y, N_iters=interval, geom_margins=[], \n",
    "        initial_weights=theta_online)\n",
    "    theta_batch = perceptron_algo(\n",
    "        not_separable_x, not_separable_y, N_iters=interval, geom_margins=[],\n",
    "        initial_weights=theta_batch, batch=True)\n",
    "    plot_line(axs[0], color=online_cmap((iter+1)/N_iters), weights=theta_online)\n",
    "    plot_line(axs[1], color=batch_cmap((iter+1)/N_iters), weights=theta_batch)\n",
    "fig.colorbar(cm.ScalarMappable(\n",
    "    norm=mpl.colors.Normalize(vmin=0, vmax=N_iters*interval-1), cmap=online_cmap), ax=axs[0])\n",
    "fig.colorbar(cm.ScalarMappable(\n",
    "    norm=mpl.colors.Normalize(vmin=0, vmax=N_iters*interval-1), cmap=batch_cmap), ax=axs[1])\n",
    "plt.savefig(os.path.join(img_folder, 'Q1_d.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763549cc",
   "metadata": {},
   "source": [
    "<font size=\"3\"> **[Question]**: Compare the results obtained using batch learning and online learning. Briefly explain the reason.\n",
    "<br> **[Answer]**: \n",
    "</font>\n",
    "\n",
    "<font size=\"3\"> **[Question]**: If batch learning is not available, is there any other tricks that might mitigate the oscillation?\n",
    "<br> **[Answer]**: \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d75963",
   "metadata": {},
   "source": [
    "## Question 2: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9564ad42",
   "metadata": {},
   "source": [
    "### preamble:\n",
    "first load the separable dataset of 200 instances of 2D-features (with offset). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777f5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sparable samples\n",
    "data_q2_separable = np.load('datasets/q2_separable.npz')\n",
    "separable_x2 = data_q2_separable['x']\n",
    "separable_y2 = data_q2_separable['y']\n",
    "\n",
    "# visualize the datapoints\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, separable_x2, separable_y2, 'separable samples (offset)', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cde035",
   "metadata": {},
   "source": [
    "### Q2(a) Primal SVM\n",
    "<font size=\"3\"> In this section, please complete the hard SVM primal problem solver 'solve_SVM_primal' (TODO marked as 'Q2(a)' in q2.py). For hard SVM, the default value of the argument 'regularization' is None. After you have completed the task, run the block below to check the decision boundary obtained. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9f952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2 import solve_SVM_primal\n",
    "\n",
    "w, b = solve_SVM_primal(separable_x2, separable_y2)\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, separable_x2, separable_y2, 'separable samples (offset)')\n",
    "plot_line(ax, 'green', w, bias=b)\n",
    "plt.savefig(os.path.join(img_folder, 'Q2_a.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da6a58",
   "metadata": {},
   "source": [
    "### Q2(b) Support Vectors\n",
    "<font size=\"3\"> To visualize the support vector, finish implementing the method 'get\\_support\\_vector'. Note that to deal with numerical errors, two numbers are considered equal when their difference is within $\\epsilon$. Run the block of plotting to verify your implementation.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf048f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from q2 import get_support_vectors\n",
    "\n",
    "positive_vectors, positive_boundary, negative_vectors, negative_boundary = get_support_vectors(\n",
    "    separable_x2, separable_y2, w, b)\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, separable_x2, separable_y2, 'check support vectors')\n",
    "plot_line(ax, 'green', w, bias=b)\n",
    "plot_line(ax, 'pink', positive_boundary[0], bias=positive_boundary[1])\n",
    "plot_line(ax, 'yellow', negative_boundary[0], bias=negative_boundary[1])\n",
    "ax.scatter(positive_vectors[:, 0], positive_vectors[:, 1], s=80, facecolors='none', edgecolors='pink')\n",
    "ax.scatter(negative_vectors[:, 0], negative_vectors[:, 1], s=80, facecolors='none', edgecolors='y')\n",
    "plt.savefig(os.path.join(img_folder, 'Q2_b.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f86796",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the nonseparable samples\n",
    "data_q2_nonseparable = np.load('datasets/q2_non_separable.npz')\n",
    "nonseparable_x2 = data_q2_nonseparable['x']\n",
    "nonseparable_y2 = data_q2_nonseparable['y']\n",
    "\n",
    "# visualize the datapoints\n",
    "fig, ax = plt.subplots(1)\n",
    "plot_samples(ax, nonseparable_x2, nonseparable_y2, 'non-separable samples (offset)', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2a2aa",
   "metadata": {},
   "source": [
    "<font size=\"3\"> In this section you need to implement the soft SVM with not-null regularization in the method 'solve\\_SVM\\_primal' in 'q2.py'. After you've completed the code, run the experiment on different regularization.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdaceef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now test the results with different regularizations\n",
    "regularozations = [0.1, 0.5, 5]\n",
    "fig, axs = plt.subplots(1, len(regularozations), figsize=(len(regularozations)*4, 3))\n",
    "for ax, C in zip(axs, regularozations):\n",
    "    # compute the result\n",
    "    w, b = solve_SVM_primal(nonseparable_x2, nonseparable_y2, regularization=C)\n",
    "    plot_samples(ax, nonseparable_x2, nonseparable_y2, f'regularization={C}')\n",
    "    plot_line(ax, 'green', w, bias=b)\n",
    "    # mark support vectors\n",
    "    positive_vectors, positive_boundary, negative_vectors, negative_boundary = get_support_vectors(\n",
    "        nonseparable_x2, nonseparable_y2, w, b)\n",
    "    plot_line(ax, 'pink', positive_boundary[0], bias=positive_boundary[1])\n",
    "    plot_line(ax, 'yellow', negative_boundary[0], bias=negative_boundary[1])\n",
    "    ax.scatter(positive_vectors[:, 0], positive_vectors[:, 1], s=80, facecolors='none', edgecolors='pink')\n",
    "    ax.scatter(negative_vectors[:, 0], negative_vectors[:, 1], s=80, facecolors='none', edgecolors='y')\n",
    "    plt.savefig(os.path.join(img_folder, 'Q2_c.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78941203",
   "metadata": {},
   "source": [
    "### Dual SVM and Cross Validation\n",
    "<font size=\"3\"> In some cases, rather than the feature vectors we only have pairwise distances/similarities of the samples. We will explore kernel transformation and dual SVM in the following section.\n",
    "In this section we use the wine data from [link](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html) and only keep two classes. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192b1ff",
   "metadata": {},
   "source": [
    "#### Kernel Functions\n",
    "<font size='3'> kernel transformation: the pairwise relations between samples are more obvious after certain kernel transformation is applied. For this question, you need to implement the RBF kernel function ($\\mathcal{K}(x, y) = exp(-\\gamma||x-y||^2) $). Finish implementing the method 'get\\_affinity\\_matrix' in `q2.py`. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c46666b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from q2 import get_affinity_matrix\n",
    "from utils import plot_affinities\n",
    "\n",
    "wine_data = np.load('datasets/wine.npz')\n",
    "wine_x = wine_data['x']\n",
    "wine_y = wine_data['y']\n",
    "transformed_basic = get_affinity_matrix(wine_x, method='product')\n",
    "transformed_rbf = get_affinity_matrix(wine_x, method='rbf', gamma=1e-5)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "plot_affinities(axs[0], transformed_basic, wine_y, title='dot product')\n",
    "plot_affinities(axs[1], transformed_rbf, wine_y, title='rbf kernel')\n",
    "plt.savefig(os.path.join(img_folder, 'Q2_d.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755ce7f",
   "metadata": {},
   "source": [
    "<font size=\"3\">Now implement the dual method and test it on the rbf embeddings. You should expect the best average test accuracy above 90% </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2 import solve_SVM_dual\n",
    "\n",
    "regularizations = [1, 5, 10]\n",
    "for regularization in regularizations:\n",
    "    print(f\"==========start the experiment on C={regularization}==========\")\n",
    "    solve_SVM_dual(transformed_rbf, wine_y, regularization=regularization, folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c835f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
